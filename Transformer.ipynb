{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyzmphGtP9rZ",
        "outputId": "18479f98-ce0c-4819-ba64-e4d6c1ac87ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "loaded 338025 tokens\n",
            "1 epoch = 165 batches\n",
            "step 0/5000 | loss 10.9527 | lr 1.200000e-06\n",
            "step 50/5000 | loss 8.3372 | lr 3.120000e-05\n",
            "step 100/5000 | loss 6.3455 | lr 6.120000e-05\n",
            "step 150/5000 | loss 6.0740 | lr 9.120000e-05\n",
            "step 200/5000 | loss 5.2532 | lr 1.212000e-04\n",
            "step 250/5000 | loss 5.3142 | lr 1.512000e-04\n",
            "step 300/5000 | loss 4.7946 | lr 1.812000e-04\n",
            "step 350/5000 | loss 5.0287 | lr 2.112000e-04\n",
            "step 400/5000 | loss 4.6069 | lr 2.412000e-04\n",
            "step 450/5000 | loss 4.8475 | lr 2.712000e-04\n",
            "step 500/5000 | loss 4.9737 | lr 3.000000e-04\n",
            "step 550/5000 | loss 4.7973 | lr 2.999049e-04\n",
            "step 600/5000 | loss 4.9622 | lr 2.996273e-04\n",
            "step 650/5000 | loss 3.9560 | lr 2.991673e-04\n",
            "step 700/5000 | loss 4.4178 | lr 2.985256e-04\n",
            "step 750/5000 | loss 4.0366 | lr 2.977029e-04\n",
            "step 800/5000 | loss 3.6059 | lr 2.967003e-04\n",
            "step 850/5000 | loss 3.8397 | lr 2.955190e-04\n",
            "step 900/5000 | loss 4.1377 | lr 2.941604e-04\n",
            "step 950/5000 | loss 4.0849 | lr 2.926261e-04\n",
            "step 1000/5000 | loss 3.5316 | lr 2.909180e-04\n",
            "step 1050/5000 | loss 3.5946 | lr 2.890383e-04\n",
            "step 1100/5000 | loss 3.9135 | lr 2.869892e-04\n",
            "step 1150/5000 | loss 3.7969 | lr 2.847732e-04\n",
            "step 1200/5000 | loss 3.4448 | lr 2.823929e-04\n",
            "step 1250/5000 | loss 3.5341 | lr 2.798514e-04\n",
            "step 1300/5000 | loss 3.3212 | lr 2.771517e-04\n",
            "step 1350/5000 | loss 3.1171 | lr 2.742970e-04\n",
            "step 1400/5000 | loss 2.9183 | lr 2.712910e-04\n",
            "step 1450/5000 | loss 2.8853 | lr 2.681371e-04\n",
            "step 1500/5000 | loss 2.6886 | lr 2.648393e-04\n",
            "step 1550/5000 | loss 2.9045 | lr 2.614016e-04\n",
            "step 1600/5000 | loss 2.9311 | lr 2.578282e-04\n",
            "step 1650/5000 | loss 2.9657 | lr 2.541234e-04\n",
            "step 1700/5000 | loss 2.4980 | lr 2.502917e-04\n",
            "step 1750/5000 | loss 2.2236 | lr 2.463379e-04\n",
            "step 1800/5000 | loss 2.2177 | lr 2.422667e-04\n",
            "step 1850/5000 | loss 1.9783 | lr 2.380830e-04\n",
            "step 1900/5000 | loss 1.9466 | lr 2.337921e-04\n",
            "step 1950/5000 | loss 1.8643 | lr 2.293991e-04\n",
            "step 2000/5000 | loss 1.8911 | lr 2.249093e-04\n",
            "step 2050/5000 | loss 1.7091 | lr 2.203283e-04\n",
            "step 2100/5000 | loss 2.0025 | lr 2.156615e-04\n",
            "step 2150/5000 | loss 1.5022 | lr 2.109148e-04\n",
            "step 2200/5000 | loss 1.5125 | lr 2.060939e-04\n",
            "step 2250/5000 | loss 1.4013 | lr 2.012046e-04\n",
            "step 2300/5000 | loss 1.1199 | lr 1.962529e-04\n",
            "step 2350/5000 | loss 1.1050 | lr 1.912449e-04\n",
            "step 2400/5000 | loss 0.9787 | lr 1.861867e-04\n",
            "step 2450/5000 | loss 1.0258 | lr 1.810843e-04\n",
            "step 2500/5000 | loss 0.7873 | lr 1.759441e-04\n",
            "step 2550/5000 | loss 0.7857 | lr 1.707723e-04\n",
            "step 2600/5000 | loss 0.8596 | lr 1.655751e-04\n",
            "step 2650/5000 | loss 0.6774 | lr 1.603590e-04\n",
            "step 2700/5000 | loss 0.5623 | lr 1.551303e-04\n",
            "step 2750/5000 | loss 0.6144 | lr 1.498953e-04\n",
            "step 2800/5000 | loss 0.5100 | lr 1.446604e-04\n",
            "step 2850/5000 | loss 0.3725 | lr 1.394321e-04\n",
            "step 2900/5000 | loss 0.3978 | lr 1.342166e-04\n",
            "step 2950/5000 | loss 0.3196 | lr 1.290203e-04\n",
            "step 3000/5000 | loss 0.2752 | lr 1.238497e-04\n",
            "step 3050/5000 | loss 0.1765 | lr 1.187108e-04\n",
            "step 3100/5000 | loss 0.1818 | lr 1.136101e-04\n",
            "step 3150/5000 | loss 0.1945 | lr 1.085537e-04\n",
            "step 3200/5000 | loss 0.1810 | lr 1.035479e-04\n",
            "step 3250/5000 | loss 0.1224 | lr 9.859859e-05\n",
            "step 3300/5000 | loss 0.1454 | lr 9.371193e-05\n",
            "step 3350/5000 | loss 0.0795 | lr 8.889385e-05\n",
            "step 3400/5000 | loss 0.0502 | lr 8.415022e-05\n",
            "step 3450/5000 | loss 0.0600 | lr 7.948682e-05\n",
            "step 3500/5000 | loss 0.0542 | lr 7.490933e-05\n",
            "step 3550/5000 | loss 0.1049 | lr 7.042332e-05\n",
            "step 3600/5000 | loss 0.0620 | lr 6.603427e-05\n",
            "step 3650/5000 | loss 0.0616 | lr 6.174751e-05\n",
            "step 3700/5000 | loss 0.0267 | lr 5.756828e-05\n",
            "step 3750/5000 | loss 0.0424 | lr 5.350166e-05\n",
            "step 3800/5000 | loss 0.0357 | lr 4.955261e-05\n",
            "step 3850/5000 | loss 0.0214 | lr 4.572594e-05\n",
            "step 3900/5000 | loss 0.0226 | lr 4.202631e-05\n",
            "step 3950/5000 | loss 0.0198 | lr 3.845823e-05\n",
            "step 4000/5000 | loss 0.0135 | lr 3.502605e-05\n",
            "step 4050/5000 | loss 0.0127 | lr 3.173394e-05\n",
            "step 4100/5000 | loss 0.0161 | lr 2.858593e-05\n",
            "step 4150/5000 | loss 0.0188 | lr 2.558584e-05\n",
            "step 4200/5000 | loss 0.0172 | lr 2.273732e-05\n",
            "step 4250/5000 | loss 0.0182 | lr 2.004386e-05\n",
            "step 4300/5000 | loss 0.0130 | lr 1.750873e-05\n",
            "step 4350/5000 | loss 0.0141 | lr 1.513502e-05\n",
            "step 4400/5000 | loss 0.0190 | lr 1.292562e-05\n",
            "step 4450/5000 | loss 0.0170 | lr 1.088323e-05\n",
            "step 4500/5000 | loss 0.0181 | lr 9.010325e-06\n",
            "step 4550/5000 | loss 0.0145 | lr 7.309197e-06\n",
            "step 4600/5000 | loss 0.0096 | lr 5.781916e-06\n",
            "step 4650/5000 | loss 0.0198 | lr 4.430343e-06\n",
            "step 4700/5000 | loss 0.0152 | lr 3.256123e-06\n",
            "step 4750/5000 | loss 0.0104 | lr 2.260689e-06\n",
            "step 4800/5000 | loss 0.0202 | lr 1.445252e-06\n",
            "step 4850/5000 | loss 0.0173 | lr 8.108059e-07\n",
            "step 4900/5000 | loss 0.0132 | lr 3.581240e-07\n",
            "step 4950/5000 | loss 0.0190 | lr 8.775781e-08\n",
            "step 4999/5000 | loss 0.0109 | lr 0.000000e+00\n",
            "final loss: 0.010906321927905083\n",
            "Saved model + config to hf_export/\n"
          ]
        }
      ],
      "source": [
        "# train_colab.py\n",
        "# Complete training script (warmup + cosine annealing + grad clipping + save export)\n",
        "# Assumes you have input.txt in the working directory.\n",
        "\n",
        "import os\n",
        "import math\n",
        "import json\n",
        "from dataclasses import dataclass, asdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Model components\n",
        "# -----------------------------\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # flag used for scaled init in GPT._init_weights\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"bias\",\n",
        "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                .view(1, 1, config.block_size, config.block_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc   = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu   = nn.GELU(approximate=\"tanh\")\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp  = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte  = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe  = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h    = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight tying\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # init weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, (\n",
        "            f\"Seq len {T} > block_size {self.config.block_size}\"\n",
        "        )\n",
        "\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "        tok_emb = self.transformer.wte(idx)      # (B, T, C)\n",
        "        pos_emb = self.transformer.wpe(pos)      # (T, C)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)                # (B, T, vocab)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                targets.view(-1)\n",
        "            )\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Data loader\n",
        "# -----------------------------\n",
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T, enc, path=\"input.txt\"):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "        self.enc = enc\n",
        "\n",
        "        if not os.path.exists(path):\n",
        "            raise FileNotFoundError(\n",
        "                f\"{path} not found. Upload/put your corpus as input.txt\"\n",
        "            )\n",
        "\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "        print(f\"loaded {len(self.tokens)} tokens\")\n",
        "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
        "\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "\n",
        "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
        "        if buf.size(0) < B*T + 1:\n",
        "            self.current_position = 0\n",
        "            buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
        "\n",
        "        x = buf[:-1].view(B, T)\n",
        "        y = buf[1:].view(B, T)\n",
        "\n",
        "        self.current_position += B*T\n",
        "        if self.current_position + (B*T + 1) > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "\n",
        "        return x, y\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Train + save\n",
        "# -----------------------------\n",
        "\n",
        "def main():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    torch.manual_seed(1337)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(1337)\n",
        "\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # ---- model config (GPT-2 small = 124M) ----\n",
        "    config = GPTConfig(\n",
        "        block_size=1024,\n",
        "        vocab_size=50257,\n",
        "        n_layer=12,\n",
        "        n_head=12,\n",
        "        n_embd=768\n",
        "    )\n",
        "\n",
        "    model = GPT(config).to(device)\n",
        "\n",
        "    # ---- data loader ----\n",
        "    train_loader = DataLoaderLite(B=8, T=256, enc=enc, path=\"input.txt\")\n",
        "\n",
        "    # ---- training hyperparams ----\n",
        "    base_lr = 3e-4\n",
        "    total_steps = 5000\n",
        "    warmup_steps = 500\n",
        "    max_grad_norm = 1.0\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr)\n",
        "\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps:\n",
        "            return float(step + 1) / float(warmup_steps)\n",
        "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "\n",
        "    model.train()\n",
        "    last_loss = None\n",
        "\n",
        "    for step in range(total_steps):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        _, loss = model(x, y)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        last_loss = loss.item()\n",
        "        if step % 50 == 0 or step == total_steps - 1:\n",
        "            print(\n",
        "                f\"step {step}/{total_steps} | \"\n",
        "                f\"loss {last_loss:.4f} | \"\n",
        "                f\"lr {scheduler.get_last_lr()[0]:.6e}\"\n",
        "            )\n",
        "\n",
        "    print(\"final loss:\", last_loss)\n",
        "\n",
        "    # ---- export for HF Spaces inference ----\n",
        "    export_dir = \"hf_export\"\n",
        "    os.makedirs(export_dir, exist_ok=True)\n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(export_dir, \"model.pt\"))\n",
        "\n",
        "    with open(os.path.join(export_dir, \"config.json\"), \"w\") as f:\n",
        "        json.dump(asdict(config), f)\n",
        "\n",
        "    with open(os.path.join(export_dir, \"meta.json\"), \"w\") as f:\n",
        "        json.dump({\n",
        "            \"tokenizer\": \"gpt2\",\n",
        "            \"vocab_size\": config.vocab_size,\n",
        "            \"block_size\": config.block_size\n",
        "        }, f)\n",
        "\n",
        "    print(f\"Saved model + config to {export_dir}/\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2d5MM7t_QMAg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}